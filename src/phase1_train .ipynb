{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fda668",
   "metadata": {
    "tags": [
     "training phase1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "  data_path: final_merged_iob_dataset.json\n",
      "  model_name: bert-base-cased\n",
      "  output_dir: ./first_phase_ner_model\n",
      "  train_size: 10000\n",
      "  eval_size: 1000\n",
      "  max_length: 256\n",
      "  epochs: 5\n",
      "  batch_size: 16\n",
      "  learning_rate: 5e-05\n",
      "  weight_decay: 0.01\n",
      "  warmup_ratio: 0.1\n",
      "  grad_accum: 1\n",
      "  seed: 42\n",
      "  num_proc: 1\n",
      "Found 7 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train: 100%|██████████| 10000/10000 [00:02<00:00, 3565.60 examples/s]\n",
      "Tokenizing eval: 100%|██████████| 1000/1000 [00:00<00:00, 3717.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 → Train: 10000 | Eval: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\WIN11\\AppData\\Local\\Temp\\ipykernel_28336\\1356634887.py:203: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 12:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>0.992703</td>\n",
       "      <td>0.997002</td>\n",
       "      <td>0.994848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.999334</td>\n",
       "      <td>0.999167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.999610</td>\n",
       "      <td>0.997004</td>\n",
       "      <td>0.997668</td>\n",
       "      <td>0.997336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.999334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.999334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Evaluating best model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.0003853101225104183\n",
      "eval_accuracy: 0.9999220212102308\n",
      "eval_precision: 0.9996666666666667\n",
      "eval_recall: 0.9990006662225184\n",
      "eval_f1: 0.9993335554815062\n",
      "eval_runtime: 3.8586\n",
      "eval_samples_per_second: 259.164\n",
      "eval_steps_per_second: 16.327\n",
      "epoch: 5.0\n",
      "Artifacts saved to ./first_phase_ner_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# ---------- Data utilities ----------\n",
    "\n",
    "def load_iob_data(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Input JSON must be a list of samples.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def validate_data(data: List[Dict[str, Any]], sample_check: int = 3) -> None:\n",
    "    if len(data) == 0:\n",
    "        raise ValueError(\"Dataset is empty.\")\n",
    "    for i, ex in enumerate(data[:sample_check]):\n",
    "        if \"tokens\" not in ex or \"labels\" not in ex:\n",
    "            raise ValueError(f\"Sample {i} missing 'tokens' or 'labels' keys.\")\n",
    "        if len(ex[\"tokens\"]) != len(ex[\"labels\"]):\n",
    "            raise ValueError(f\"Sample {i} has mismatched tokens({len(ex['tokens'])}) and labels({len(ex['labels'])}).\")\n",
    "\n",
    "\n",
    "def create_label_map(dataset: List[Dict[str, Any]]):\n",
    "    unique = sorted({lab for ex in dataset for lab in ex[\"labels\"]})\n",
    "    label2id = {l: i for i, l in enumerate(unique)}\n",
    "    id2label = {i: l for l, i in label2id.items()}\n",
    "    return unique, label2id, id2label\n",
    "\n",
    "\n",
    "# ---------- Tokenization & alignment ----------\n",
    "\n",
    "def tokenize_and_align_labels(example, tokenizer, label2id, max_length: int = 256):\n",
    "    # Dynamic padding (no padding here; DataCollator will pad batches)\n",
    "    encoded = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    word_ids = encoded.word_ids()\n",
    "    labels = example[\"labels\"]\n",
    "\n",
    "    aligned_labels = []\n",
    "    prev_word_id = None\n",
    "    for w_id in word_ids:\n",
    "        if w_id is None:\n",
    "            aligned_labels.append(-100)  # special/padding tokens\n",
    "        elif w_id != prev_word_id:\n",
    "            aligned_labels.append(label2id[labels[w_id]])  # first subtoken\n",
    "        else:\n",
    "            aligned_labels.append(-100)  # subsequent subtokens\n",
    "        prev_word_id = w_id\n",
    "\n",
    "    encoded[\"labels\"] = aligned_labels\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "\n",
    "def compute_metrics(eval_pred, id2label):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for p_seq, l_seq in zip(preds, labels):\n",
    "        tl_seq = []\n",
    "        pl_seq = []\n",
    "        for p, l in zip(p_seq, l_seq):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            tl_seq.append(id2label[int(l)])\n",
    "            pl_seq.append(id2label[int(p)])\n",
    "        true_labels.append(tl_seq)\n",
    "        pred_labels.append(pl_seq)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, pred_labels),\n",
    "        \"precision\": precision_score(true_labels, pred_labels),\n",
    "        \"recall\": recall_score(true_labels, pred_labels),\n",
    "        \"f1\": f1_score(true_labels, pred_labels),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Phase 1 NER fine-tuning (10k train, 1k eval)\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"final_merged_iob_dataset.json\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"bert-base-cased\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./first_phase_ner_model\")\n",
    "    parser.add_argument(\"--train_size\", type=int, default=10_000)\n",
    "    parser.add_argument(\"--eval_size\", type=int, default=1_000)\n",
    "    parser.add_argument(\"--max_length\", type=int, default=256)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=5)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--warmup_ratio\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--grad_accum\", type=int, default=1)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--num_proc\", type=int, default=1, help=\"Set >1 to parallelize Dataset.map() if your OS supports it.\")\n",
    "    args = parser.parse_args() if hasattr(__builtins__, \"__IPYTHON__\") is False else parser.parse_args([])\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Config:\")\n",
    "    for k, v in vars(args).items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    # Reproducibility\n",
    "    set_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # Load and validate data\n",
    "    data = load_iob_data(args.data_path)\n",
    "    validate_data(data)\n",
    "\n",
    "    # Create label maps from full dataset\n",
    "    label_list, label2id, id2label = create_label_map(data)\n",
    "    print(f\"Found {len(label_list)} labels.\")\n",
    "\n",
    "    # Shuffle and slice Phase 1 subsets\n",
    "    data_shuffled = random.sample(data, len(data))\n",
    "    if len(data_shuffled) < args.train_size + args.eval_size:\n",
    "        raise ValueError(f\"Dataset too small: need {args.train_size + args.eval_size}, have {len(data_shuffled)}\")\n",
    "\n",
    "    train_raw = data_shuffled[:args.train_size]\n",
    "    eval_raw = data_shuffled[args.train_size:args.train_size + args.eval_size]\n",
    "\n",
    "    # Tokenizer and datasets\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "\n",
    "    def _map_fn(ex):\n",
    "        return tokenize_and_align_labels(ex, tokenizer, label2id, max_length=args.max_length)\n",
    "\n",
    "    train_ds = Dataset.from_list(train_raw).map(_map_fn, batched=False, num_proc=args.num_proc, desc=\"Tokenizing train\")\n",
    "    eval_ds = Dataset.from_list(eval_raw).map(_map_fn, batched=False, num_proc=args.num_proc, desc=\"Tokenizing eval\")\n",
    "\n",
    "    print(f\"Phase 1 → Train: {len(train_ds)} | Eval: {len(eval_ds)}\")\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        args.model_name,\n",
    "        num_labels=len(label_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    # Training args\n",
    "    fp16 = torch.cuda.is_available()\n",
    "    bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported() if hasattr(torch.cuda, \"is_bf16_supported\") else False\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=os.path.join(args.output_dir, \"logs\"),\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.grad_accum,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        seed=args.seed,\n",
    "        group_by_length=True,\n",
    "        fp16=fp16 and not bf16,\n",
    "        bf16=bf16,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=lambda p: compute_metrics(p, id2label),\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluating best model...\")\n",
    "    metrics = trainer.evaluate()\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Save model, tokenizer, and label maps\n",
    "    model.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    with open(os.path.join(args.output_dir, \"label_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"label_list\": label_list, \"label2id\": label2id, \"id2label\": id2label}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Artifacts saved to {args.output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b632c9",
   "metadata": {
    "tags": [
     "testing snippet"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John         → None\n",
      "is           → O\n",
      "skilled      → O\n",
      "in           → O\n",
      "Python       → O\n",
      "and          → B-SKILL\n",
      "machine      → O\n",
      "learning     → B-SKILL\n",
      "at           → I-SKILL\n",
      "DeepMind     → O\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer (use absolute path if needed)\n",
    "model_dir = \"./first_phase_ner_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_dir, local_files_only=True)\n",
    "model.eval()\n",
    "\n",
    "id2label = model.config.id2label\n",
    "\n",
    "# Sample input\n",
    "tokens = [\"John\", \"is\", \"skilled\", \"in\", \"Python\", \"and\", \"machine\", \"learning\", \"at\", \"DeepMind\"]\n",
    "encoded = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "logits = outputs.logits\n",
    "preds = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "word_ids = encoded.word_ids()\n",
    "\n",
    "# Align prediction to original tokens\n",
    "labels = []\n",
    "prev_word_id = None\n",
    "for idx, word_id in enumerate(word_ids):\n",
    "    if word_id is None or word_id == prev_word_id:\n",
    "        labels.append(None)\n",
    "    else:\n",
    "        label_id = preds[idx]\n",
    "        labels.append(id2label[label_id])\n",
    "        prev_word_id = word_id\n",
    "\n",
    "# Show result\n",
    "for token, label in zip(tokens, labels):\n",
    "    print(f\"{token:12s} → {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
