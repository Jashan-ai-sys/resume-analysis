{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ef0458",
   "metadata": {
    "tags": [
     "training t5 model"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN11\\AppData\\Local\\Temp\\ipykernel_30508\\2320645591.py:135: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1180' max='1180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1180/1180 44:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669300</td>\n",
       "      <td>0.413191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.360400</td>\n",
       "      <td>0.262749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.219035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.192850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='418' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [418/418 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Evaluation Metrics: {'eval_loss': 0.19284331798553467, 'eval_runtime': 23.5935, 'eval_samples_per_second': 17.717, 'eval_steps_per_second': 17.717, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./t5_skill_feedback\\\\tokenizer_config.json',\n",
       " './t5_skill_feedback\\\\special_tokens_map.json',\n",
       " './t5_skill_feedback\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from evaluate import load\n",
    "\n",
    "# ‚úÖ Settings\n",
    "model_name = \"t5-small\"   # try \"flan-t5-base\" if outputs collapse\n",
    "train_file = \"C:/Users/WIN11/Intelligent-Resume-Feedback-System/data/train_T5.jsonl\"\n",
    "valid_file = \"C:/Users/WIN11/Intelligent-Resume-Feedback-System/data/valid_T5.jsonl\"\n",
    "output_dir = \"./t5_skill_feedback\"\n",
    "\n",
    "# ‚úÖ Training hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 4\n",
    "grad_accum = 4\n",
    "lr = 3e-4\n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.06\n",
    "patience = 3\n",
    "seed = 42\n",
    "fp16 = True\n",
    "gradient_checkpointing = True\n",
    "eval_every_steps = 500\n",
    "save_total_limit = 2\n",
    "logging_steps = 50\n",
    "\n",
    "\n",
    "# 1) Load tokenizer & model\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "if gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2) Load dataset\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": train_file, \"validation\": valid_file}\n",
    ")\n",
    "\n",
    "# 3) Preprocess\n",
    "def preprocess(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=max_input_len,\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        max_length=max_target_len,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 4) Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 5) Metrics\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE + BLEU\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_result = bleu.compute(predictions=[p.split() for p in decoded_preds],\n",
    "                               references=[[l.split()] for l in decoded_labels])\n",
    "\n",
    "    # üîé Show a live sample after each eval\n",
    "    print(\"\\n--- Live Sample ---\")\n",
    "    sample_input = \"Company: Google | Role: Data Scientist | Candidate Skills: Python, SQL, Pandas\"\n",
    "    inputs = tokenizer(sample_input, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=max_target_len)\n",
    "    print(\"INPUT:\", sample_input)\n",
    "    print(\"OUTPUT:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    print(\"--------------------\\n\")\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"]\n",
    "    }\n",
    "\n",
    "# 6) Training arguments\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# ‚úÖ Optimized Training Arguments for 4GB GPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",          # evaluate once per epoch\n",
    "    save_strategy=\"epoch\",                # save best checkpoint per epoch\n",
    "    save_total_limit=2,                   # keep last 2 checkpoints only\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=4,        # smaller batch to fit VRAM\n",
    "    per_device_eval_batch_size=1,         # eval one sample at a time\n",
    "    gradient_accumulation_steps=4,        # simulate effective batch=16\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    logging_steps=logging_steps,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"none\"],                   # or [\"tensorboard\"] if needed\n",
    "    seed=seed,\n",
    "    fp16=True,                            # mixed precision = less VRAM\n",
    "    fp16_full_eval=True,                  # mixed precision for eval too\n",
    "    dataloader_num_workers=0,             # reduce extra memory threads\n",
    ")\n",
    "\n",
    "# ‚úÖ Early stopping (optional)\n",
    "callbacks = []\n",
    "if patience > 0:\n",
    "    callbacks.append(EarlyStoppingCallback(early_stopping_patience=patience))\n",
    "\n",
    "# ‚úÖ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# üöÄ Train\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ After training, display validation results\n",
    "metrics = trainer.evaluate()\n",
    "print(\"\\nüìä Final Evaluation Metrics:\", metrics)\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce31163",
   "metadata": {
    "tags": [
     "testing"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Testing Fine-Tuned Model\n",
      "\n",
      "Input: Company: Google | Role: Data Scientist | Candidate Skills: Python, SQL, Pandas\n",
      "Output: Strong base with Python, SQL, Pandas. It would help to work on SQL, Machine Learning, Deep Learning, TensorFlow, PyTorch, Statistics, Data Visualization, Feature Engineering, Pandas, Numpy, Model Deployment, Spark, A/B Testing, Data Cleaning. Your profile aligns 13% with the role.\n",
      "--------------------------------------------------------------------------------\n",
      "Input: Company: Microsoft | Role: Backend Developer | Candidate Skills: Python, Django, SQL\n",
      "Output: Strong base with Python, Django, SQL. It would help to work on Java, Spring Boot, SQL, Docker, Microservices, APIs, Python, System Design, NoSQL (MongoDB), GraphQL, Scalability, Kafka, Redis, CI/CD, Cloud Platforms. Your profile aligns 13% with the role.\n",
      "--------------------------------------------------------------------------------\n",
      "Input: Company: Accenture | Role: Data Scientist | Candidate Skills: Numpy, TensorFlow, SQL, Pandas, Deep Learning\n",
      "Output: Strong base with Numpy, TensorFlow, SQL, Pandas, Deep Learning. Consider strengthening Python, SQL, Machine Learning, PyTorch, Statistics, Data Visualization, Feature Engineering, Pandas, Model Deployment, Spark, A/B Testing for better fit. Your profile aligns 40% with the role.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "# ‚úÖ Load fine-tuned model + tokenizer\n",
    "model_path = \"./t5_skill_feedback\"   # change if saved elsewhere\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# ‚úÖ Function for inference\n",
    "def generate_feedback(input_text, max_length=128):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=4,           # beam search for better text\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ‚úÖ Test samples\n",
    "samples = [\n",
    "    \"Company: Google | Role: Data Scientist | Candidate Skills: Python, SQL, Pandas\",\n",
    "    \"Company: Microsoft | Role: Backend Developer | Candidate Skills: Python, Django, SQL\",\n",
    "    \"Company: Accenture | Role: Data Scientist | Candidate Skills: Numpy, TensorFlow, SQL, Pandas, Deep Learning\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîé Testing Fine-Tuned Model\\n\")\n",
    "for s in samples:\n",
    "    print(\"Input:\", s)\n",
    "    print(\"Output:\", generate_feedback(s))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e51fe56b",
   "metadata": {
    "tags": [
     "post processing "
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Input: Google | Data Scientist | ['Python', 'SQL', 'Pandas']\n",
      "üìù Raw Model Output: You already have good expertise in Python, SQL, Pandas. It would help to work on SQL, Machine Learning, Deep Learning, TensorFlow, PyTorch, Statistics, Data Visualization, Feature Engineering, Pandas, Numpy, Model Deployment, Spark, A/B Testing, Data Cleaning. You meet around 26% of the requirements.\n",
      "‚úÖ Final Feedback:\n",
      " ‚úÖ You already have Python, SQL, Pandas.\n",
      "üìå To improve your profile for Data Scientist, focus on learning: Machine Learning, Deep Learning, TensorFlow, PyTorch, Statistics, Data Visualization, Feature Engineering, Numpy, Model Deployment, Spark, A/B Testing, Data Cleaning.\n",
      "üìä Profile Match: 20.0%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîπ Input: Microsoft | Backend Developer | ['Python', 'Django', 'SQL']\n",
      "üìù Raw Model Output: You already have good expertise in Python, Django, SQL. It would help to work on Java, Spring Boot, SQL, Docker, Microservices, APIs, Python, System Design, NoSQL (MongoDB), GraphQL, Scalability, Kafka, Redis, CI/CD, Cloud Platforms. Your profile aligns 13% with the role.\n",
      "‚úÖ Final Feedback:\n",
      " ‚úÖ You already have Python, Django, SQL.\n",
      "üìå To improve your profile for Backend Developer, focus on learning: Java, Spring Boot, Docker, Microservices, APIs, System Design, NoSQL (MongoDB), GraphQL, Scalability, Kafka, Redis, CI/CD, Cloud Platforms.\n",
      "üìä Profile Match: 13.33%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîπ Input: Accenture | Data Scientist | ['Numpy', 'TensorFlow', 'SQL', 'Pandas', 'Deep Learning']\n",
      "üìù Raw Model Output: Good foundation in Numpy, TensorFlow, SQL, Pandas, Deep Learning. Still missing Python, SQL, Machine Learning, PyTorch, Statistics, Data Visualization, Feature Engineering, Pandas, Model Deployment, Spark, A/B Testing, Data Cleaning, which are important for this role. Your profile aligns 40% with the role.\n",
      "‚úÖ Final Feedback:\n",
      " ‚úÖ You already have Numpy, TensorFlow, SQL, Pandas, Deep Learning.\n",
      "üìå To improve your profile for Data Scientist, focus on learning: Python, Machine Learning, PyTorch, Statistics, Data Visualization, Feature Engineering, Model Deployment, Spark, A/B Testing, Data Cleaning.\n",
      "üìä Profile Match: 33.33%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "import json\n",
    "\n",
    "# ===============================\n",
    "# üîπ 1. Load fine-tuned model\n",
    "# ===============================\n",
    "MODEL_DIR = r\"C:/Users/WIN11/Intelligent-Resume-Feedback-System/src/t5_skill_feedback\"\n",
    "SKILL_DICT_FILE = r\"C:/Users/WIN11/Intelligent-Resume-Feedback-System/data/skill_requirement_dataset.json\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5TokenizerFast.from_pretrained(MODEL_DIR)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_DIR).to(device)\n",
    "\n",
    "# ===============================\n",
    "# üîπ 2. Load skill dictionary JSON\n",
    "# ===============================\n",
    "with open(SKILL_DICT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    skill_data = json.load(f)   # <-- directly load array\n",
    "\n",
    "# Convert into role ‚Üí skills dict\n",
    "skill_dict = {}\n",
    "for company, roles in skill_data.items():\n",
    "    for role, skills in roles.items():\n",
    "        skill_dict[f\"{company}|{role}\"] = skills# ===============================\n",
    "# üîπ 3. Generate raw T5 feedback\n",
    "# ===============================\n",
    "def generate_feedback(company, role, candidate_skills):\n",
    "    input_text = f\"Company: {company} | Role: {role} | Candidate Skills: {', '.join(candidate_skills)}\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    outputs = model.generate(inputs, max_length=128, num_beams=4, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ===============================\n",
    "# üîπ 4. Post-process feedback\n",
    "# ===============================\n",
    "def post_process_feedback(candidate_skills, role, raw_output):\n",
    "    required_skills = skill_dict.get(f\"{company}|{role}\", [])\n",
    "    cand_set = set([s.lower() for s in candidate_skills])\n",
    "    req_set = set([s.lower() for s in required_skills])\n",
    "\n",
    "    # Missing skills = required - candidate\n",
    "    missing = [s for s in required_skills if s.lower() not in cand_set]\n",
    "\n",
    "    # Compute match %\n",
    "    match_percent = round(len(req_set & cand_set) / len(req_set) * 100, 2) if req_set else 0\n",
    "\n",
    "    # Build clean feedback\n",
    "    feedback = (\n",
    "        f\"‚úÖ You already have {', '.join(candidate_skills)}.\\n\"\n",
    "        f\"üìå To improve your profile for {role}, focus on learning: {', '.join(missing) if missing else 'No extra skills needed!'}.\\n\"\n",
    "        f\"üìä Profile Match: {match_percent}%\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"candidate_skills\": candidate_skills,\n",
    "        \"required_skills\": required_skills,\n",
    "        \"missing_skills\": missing,\n",
    "        \"match_percent\": match_percent,\n",
    "        \"raw_output\": raw_output,\n",
    "        \"final_feedback\": feedback\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# üîπ 5. Test\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    tests = [\n",
    "        (\"Google\", \"Data Scientist\", [\"Python\", \"SQL\", \"Pandas\"]),\n",
    "        (\"Microsoft\", \"Backend Developer\", [\"Python\", \"Django\", \"SQL\"]),\n",
    "        (\"Accenture\", \"Data Scientist\", [\"Numpy\", \"TensorFlow\", \"SQL\", \"Pandas\", \"Deep Learning\"]),\n",
    "    ]\n",
    "\n",
    "    for company, role, skills in tests:\n",
    "        raw = generate_feedback(company, role, skills)\n",
    "        result = post_process_feedback(skills, role, raw)\n",
    "\n",
    "        print(\"\\nüîπ Input:\", company, \"|\", role, \"|\", skills)\n",
    "        print(\"üìù Raw Model Output:\", result[\"raw_output\"])\n",
    "        print(\"‚úÖ Final Feedback:\\n\", result[\"final_feedback\"])\n",
    "        print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
