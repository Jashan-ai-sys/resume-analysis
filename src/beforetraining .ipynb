{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e1d7a",
   "metadata": {
    "tags": [
     "merging kaggle and resumes benchmark data"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def extract_skills_from_text(text):\n",
    "    \"\"\"\n",
    "    Very simple keyword-based skill extractor from text.\n",
    "    Replace this with spaCy or ML NER model for better results.\n",
    "    \"\"\"\n",
    "    skill_keywords = [\n",
    "        \"project management\", \"quality\", \"budget\", \"client\", \"proposal\",\n",
    "        \"reporting\", \"execution\", \"operations\", \"risk\", \"planning\", \"SOW\", \"RFQ\"\n",
    "    ]\n",
    "    \n",
    "    text = text.lower()\n",
    "    found_skills = []\n",
    "    for keyword in skill_keywords:\n",
    "        if keyword.lower() in text:\n",
    "            found_skills.append(keyword.lower())\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "def extract_from_experience_blocks(resume_data):\n",
    "    skills_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "    for profile in resume_data:\n",
    "        experiences = profile.get(\"experience\", [])\n",
    "        for exp in experiences:\n",
    "            raw_company = exp.get(\"company\", \"\").strip().lower()\n",
    "            company = re.sub(r'[^a-zA-Z0-9 ]', '', raw_company).strip()\n",
    "\n",
    "            role = (exp.get(\"title\") or \"\").strip().lower()\n",
    "\n",
    "            responsibilities = exp.get(\"responsibilities\", [])\n",
    "            if not responsibilities:\n",
    "                continue\n",
    "\n",
    "            for line in responsibilities:\n",
    "                inferred_skills = extract_skills_from_text(line)\n",
    "                for skill in inferred_skills:\n",
    "                    skills_dict[company][role][skill] += 1\n",
    "\n",
    "    return skills_dict\n",
    "\n",
    "def merge_skills_into_benchmark(existing_benchmark, new_skills):\n",
    "    for company, roles in new_skills.items():\n",
    "        if company not in existing_benchmark:\n",
    "            existing_benchmark[company] = {}\n",
    "\n",
    "        for role, skills in roles.items():\n",
    "            if role not in existing_benchmark[company]:\n",
    "                existing_benchmark[company][role] = {}\n",
    "\n",
    "            for skill, count in skills.items():\n",
    "                existing_count = existing_benchmark[company][role].get(skill, 0)\n",
    "                existing_benchmark[company][role][skill] = existing_count + count\n",
    "\n",
    "    return existing_benchmark\n",
    "\n",
    "# === Paths ===\n",
    "resume_path = \"resumes_dataset.json\"\n",
    "benchmark_path = \"merged_benchmark_by_company_role.json\"\n",
    "output_path = \"merged_benchmark_by_company_role.json\"\n",
    "\n",
    "# === Pipeline ===\n",
    "print(\"üìÇ Loading files...\")\n",
    "resume_data = load_json(resume_path)\n",
    "existing_benchmark = load_json(benchmark_path)\n",
    "\n",
    "print(\"üîç Extracting from responsibilities + roles...\")\n",
    "inferred_skills = extract_from_experience_blocks(resume_data)\n",
    "\n",
    "print(\"üîó Merging into benchmark...\")\n",
    "updated = merge_skills_into_benchmark(existing_benchmark, inferred_skills)\n",
    "\n",
    "print(\"üíæ Saving merged file...\")\n",
    "save_json(updated, output_path)\n",
    "\n",
    "print(\"‚úÖ Resume experiences merged into benchmark.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615d7f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 1440 labeled samples ‚Üí benchmark_iob_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ==== CONFIG ====\n",
    "benchmark_file = \"kaggle_resume_benchmarkdata.json\"  # Your benchmark data file\n",
    "output_file = \"benchmark_iob_dataset.json\"           # Output IOB dataset\n",
    "pretrained_model = \"bert-base-cased\"                 # Match your NER model tokenizer\n",
    "samples_per_role = 5                                 # How many synthetic sentences per role\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# Load benchmark data\n",
    "with open(benchmark_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "# Sentence templates\n",
    "role_templates = [\n",
    "    \"Worked as a {role} at {company} specializing in {skill}.\",\n",
    "    \"Served as {role} for {company} with expertise in {skill}.\",\n",
    "    \"Held the position of {role} at {company}, focusing on {skill}.\",\n",
    "    \"As a {role}, contributed to {company} using {skill}.\",\n",
    "    \"Responsible for {skill} while working as a {role} at {company}.\"\n",
    "]\n",
    "\n",
    "# Tokenize & label\n",
    "def tokenize_and_label(sentence, role, company, skill):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    def label_entity(entity, label_prefix):\n",
    "        entity_tokens = tokenizer.tokenize(entity)\n",
    "        for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "            if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                labels[i] = f\"B-{label_prefix}\"\n",
    "                for j in range(1, len(entity_tokens)):\n",
    "                    labels[i+j] = f\"I-{label_prefix}\"\n",
    "\n",
    "    label_entity(company, \"COMPANY\")\n",
    "    label_entity(role, \"ROLE\")\n",
    "    label_entity(skill, \"SKILL\")\n",
    "    return tokens, labels\n",
    "\n",
    "# Build dataset\n",
    "iob_dataset = []\n",
    "\n",
    "# Detect if data is dict or list\n",
    "if isinstance(benchmark_data, dict):\n",
    "    company_items = benchmark_data.items()\n",
    "else:\n",
    "    company_items = [(entry.get(\"company\"), entry.get(\"roles\", [])) for entry in benchmark_data]\n",
    "\n",
    "for company, roles in company_items:\n",
    "    if not company or not roles:\n",
    "        continue\n",
    "\n",
    "    for role_entry in roles:\n",
    "        # Role might be a dict or string\n",
    "        if isinstance(role_entry, dict):\n",
    "            role = role_entry.get(\"role\")\n",
    "            skills = role_entry.get(\"skills\", [])\n",
    "        else:\n",
    "            role = str(role_entry)\n",
    "            skills = []\n",
    "\n",
    "        if not role:\n",
    "            continue\n",
    "\n",
    "        # If no skills, still make a generic skill placeholder\n",
    "        if not skills:\n",
    "            skills = [\"technology\", \"software development\"]\n",
    "\n",
    "        for skill in skills:\n",
    "            for _ in range(samples_per_role):\n",
    "                template = random.choice(role_templates)\n",
    "                sentence = template.format(role=role, company=company, skill=skill)\n",
    "                tokens, labels = tokenize_and_label(sentence, role, company, skill)\n",
    "                iob_dataset.append({\"tokens\": tokens, \"labels\": labels})\n",
    "\n",
    "# Save to file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(iob_dataset, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(iob_dataset)} labeled samples ‚Üí {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b88642",
   "metadata": {
    "tags": [
     "synthetic data generator "
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Augmented dataset saved to benchmark_iob_dataset_augmented.json\n",
      "Original dataset size: 1440\n",
      "Synthetic dataset size: 113925\n",
      "Total dataset size: 115365\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Config\n",
    "# ===============================\n",
    "benchmark_file = \"benchmark_iob_dataset.json\"\n",
    "companies_file = \"extra_companies.json\"\n",
    "roles_file = \"extra_roles.json\"\n",
    "skills_file = \"extra_skills.json\"\n",
    "output_file = \"benchmark_iob_dataset_augmented.json\"\n",
    "samples_per_triplet = 5  # how many variations per (company, role, skill)\n",
    "\n",
    "# ===============================\n",
    "# Load spaCy model\n",
    "# ===============================\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Helper function using spaCy for IOB labeling\n",
    "# ===============================\n",
    "import re\n",
    "\n",
    "def tokenize_and_label(sentence, role, company, skill):\n",
    "    \"\"\"\n",
    "    Tokenizes and labels the sentence with IOB tags for COMPANY, ROLE, SKILL.\n",
    "    - Case-insensitive\n",
    "    - Longest-first entity matching\n",
    "    - Strips punctuation from tokens\n",
    "    - Avoids overlapping spans\n",
    "    \"\"\"\n",
    "\n",
    "    # ====== Tokenize ======\n",
    "    tokens = re.findall(r\"\\w+|\\S\", sentence)  # Keeps punctuation as separate tokens\n",
    "    norm_tokens = [t.lower() for t in tokens]\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    # ====== Normalize Entities ======\n",
    "    def normalize(text):\n",
    "        return [w.lower() for w in re.findall(r\"\\w+|\\S\", text)]\n",
    "\n",
    "    entities = [\n",
    "        (normalize(company), \"COMPANY\"),\n",
    "        (normalize(role), \"ROLE\"),\n",
    "        (normalize(skill), \"SKILL\"),\n",
    "    ]\n",
    "\n",
    "    # Sort longest-first to prefer multi-word matches\n",
    "    entities.sort(key=lambda e: len(e[0]), reverse=True)\n",
    "\n",
    "    # ====== Match and Label ======\n",
    "    for ent_tokens, ent_type in entities:\n",
    "        L = len(ent_tokens)\n",
    "        if L == 0:\n",
    "            continue\n",
    "        for i in range(len(norm_tokens) - L + 1):\n",
    "            window = norm_tokens[i:i+L]\n",
    "            if window == ent_tokens and all(lbl == \"O\" for lbl in labels[i:i+L]):\n",
    "                labels[i] = f\"B-{ent_type}\"\n",
    "                for j in range(1, L):\n",
    "                    labels[i + j] = f\"I-{ent_type}\"\n",
    "                break\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Load datasets\n",
    "# ===============================\n",
    "with open(benchmark_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "with open(companies_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    extra_companies = json.load(f)\n",
    "\n",
    "with open(roles_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    extra_roles = json.load(f)\n",
    "\n",
    "with open(skills_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    extra_skills = json.load(f)\n",
    "\n",
    "# ===============================\n",
    "# Templates for synthetic sentences\n",
    "# ===============================\n",
    "templates = [\n",
    "    \"Worked as a {role} at {company} specializing in {skill}.\",\n",
    "    \"Served as {role} for {company} with expertise in {skill}.\",\n",
    "    \"Held the position of {role} at {company}, focusing on {skill}.\",\n",
    "    \"As a {role}, contributed to {company} using {skill}.\",\n",
    "    \"Responsible for {skill} development at {company} as a {role}.\",\n",
    "    \"Implemented {skill} solutions while working as a {role} at {company}.\",\n",
    "    \"Developed {skill} projects at {company} in the role of {role}.\"\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Generate synthetic dataset\n",
    "# ===============================\n",
    "synthetic_data = []\n",
    "\n",
    "for company in extra_companies:\n",
    "    for role in extra_roles:\n",
    "        for skill in extra_skills:\n",
    "            for _ in range(samples_per_triplet):\n",
    "                template = random.choice(templates)\n",
    "                sentence = template.format(role=role, company=company, skill=skill)\n",
    "                tokens, labels = tokenize_and_label(sentence, role, company, skill)\n",
    "                synthetic_data.append({\n",
    "                    \"tokens\": tokens,\n",
    "                    \"labels\": labels\n",
    "                })\n",
    "\n",
    "# ===============================\n",
    "# Merge with benchmark\n",
    "# ===============================\n",
    "augmented_dataset = benchmark_data + synthetic_data\n",
    "\n",
    "# ===============================\n",
    "# Save output\n",
    "# ===============================\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(augmented_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Augmented dataset saved to {output_file}\")\n",
    "print(f\"Original dataset size: {len(benchmark_data)}\")\n",
    "print(f\"Synthetic dataset size: {len(synthetic_data)}\")\n",
    "print(f\"Total dataset size: {len(augmented_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf482c7",
   "metadata": {
    "tags": [
     "merge_ori_synthetic_dataset"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 1440\n",
      "Synthetic dataset size: 115365\n",
      "‚úÖ Merged dataset saved to final_merged_iob_dataset.json\n",
      "Total samples in merged dataset: 116805\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# ===============================\n",
    "# File paths (update if needed)\n",
    "# ===============================\n",
    "original_file = \"benchmark_iob_dataset.json\"  # Your original dataset\n",
    "synthetic_file = \"benchmark_iob_dataset_augmented.json\"     # Your generated synthetic dataset\n",
    "merged_file = \"final_merged_iob_dataset.json\"       # Output merged dataset\n",
    "\n",
    "# ===============================\n",
    "# Load datasets\n",
    "# ===============================\n",
    "with open(original_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "with open(synthetic_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    synthetic_data = json.load(f)\n",
    "\n",
    "print(f\"Original dataset size: {len(original_data)}\")\n",
    "print(f\"Synthetic dataset size: {len(synthetic_data)}\")\n",
    "\n",
    "# ===============================\n",
    "# Merge and shuffle\n",
    "# ===============================\n",
    "merged_data = original_data + synthetic_data\n",
    "random.shuffle(merged_data)\n",
    "\n",
    "# ===============================\n",
    "# Save merged dataset\n",
    "# ===============================\n",
    "with open(merged_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Merged dataset saved to {merged_file}\")\n",
    "print(f\"Total samples in merged dataset: {len(merged_data)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
