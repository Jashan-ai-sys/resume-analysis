{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ba7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Synthetic NER dataset generated and saved as synthetic_ner_data.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Define sample entities\n",
    "# -------------------------------\n",
    "indian_names = [\"John Doe\", \"Jane Smith\", \"Rahul Sharma\", \"Priya Singh\",\n",
    "    \"Ankit Kumar\", \"Neha Patel\", \"Vikram Joshi\", \"Aisha Khan\"]\n",
    "roles = [\n",
    "    \"Senior AI/ML Engineer\", \"Full-stack Developer\", \"Cloud Solutions Architect\",\n",
    "    \"Data Scientist\", \"Machine Learning Engineer\"\n",
    "]\n",
    "companies = [\n",
    "    \"DeepTech Solutions\", \"InnoData Labs\", \"CloudNova Pvt Ltd\", \"NextGen AI\", \"TechVantage\"\n",
    "]\n",
    "skills = [\n",
    "     \"Python\", \"Java\", \"C++\", \"JavaScript\", \"TypeScript\", \"Go\", \"Rust\",\n",
    "    \"TensorFlow\", \"PyTorch\", \"FastAPI\", \"Flask\", \"Docker\", \"Kubernetes\",\n",
    "    \"Git\", \"Terraform\", \"Jenkins\", \"PostgreSQL\", \"MongoDB\", \"Redis\",\n",
    "    \"Apache Kafka\", \"Spark\", \"Hadoop\", \"NLP\", \"Reinforcement Learning\", \"Distributed Systems\",\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Generate synthetic sentences\n",
    "# -------------------------------\n",
    "def generate_sentence():\n",
    "    name = random.choice(indian_names)\n",
    "    role = random.choice(roles)\n",
    "    company = random.choice(companies)\n",
    "    skill_sample = random.sample(skills, k=5)  # pick 5 random skills\n",
    "\n",
    "    sentence = f\"{name} is a {role} at {company}. Skilled in {', '.join(skill_sample)}.\"\n",
    "    \n",
    "    entities = []\n",
    "\n",
    "    # BIO tagging\n",
    "    # Name\n",
    "    name_tokens = name.split()\n",
    "    entities.extend([{\"word\": token, \"entity_group\": \"B-PER\"} if i==0 else {\"word\": token, \"entity_group\": \"I-PER\"} for i, token in enumerate(name_tokens)])\n",
    "    \n",
    "    # Role\n",
    "    role_tokens = role.split()\n",
    "    entities.extend([{\"word\": token, \"entity_group\": \"B-ROLE\"} if i==0 else {\"word\": token, \"entity_group\": \"I-ROLE\"} for i, token in enumerate(role_tokens)])\n",
    "    \n",
    "    # Company\n",
    "    company_tokens = company.split()\n",
    "    entities.extend([{\"word\": token, \"entity_group\": \"B-COMPANY\"} if i==0 else {\"word\": token, \"entity_group\": \"I-COMPANY\"} for i, token in enumerate(company_tokens)])\n",
    "    \n",
    "    # Skills\n",
    "    for skill in skill_sample:\n",
    "        skill_tokens = skill.split()\n",
    "        entities.extend([{\"word\": token, \"entity_group\": \"B-SKILL\"} if i==0 else {\"word\": token, \"entity_group\": \"I-SKILL\"} for i, token in enumerate(skill_tokens)])\n",
    "    \n",
    "    return {\"text\": sentence, \"entities\": entities}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Generate dataset\n",
    "# -------------------------------\n",
    "num_samples = 50  # generate 50 synthetic examples\n",
    "synthetic_dataset = [generate_sentence() for _ in range(num_samples)]\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Save to JSON for later use\n",
    "# -------------------------------\n",
    "with open(\"synthetic_ner_data.json\", \"w\") as f:\n",
    "    json.dump(synthetic_dataset, f, indent=4)\n",
    "\n",
    "print(\"✅ Synthetic NER dataset generated and saved as synthetic_ner_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ac813d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\WIN11\\AppData\\Local\\Temp\\ipykernel_23080\\4092599711.py:118: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 0. Imports\n",
    "# -------------------------------\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Example data\n",
    "# -------------------------------\n",
    "# Replace this with your real dataset\n",
    "data = [\n",
    "    {\"text\": \"John works at OpenAI\", \"entities\": [{\"word\": \"John\", \"label\": \"B-PER\"}, {\"word\": \"OpenAI\", \"label\": \"B-ORG\"}]},\n",
    "    {\"text\": \"Alice joined Google\", \"entities\": [{\"word\": \"Alice\", \"label\": \"B-PER\"}, {\"word\": \"Google\", \"label\": \"B-ORG\"}]}\n",
    "]\n",
    "\n",
    "# Convert B-PER/I-PER to O if not trained\n",
    "for item in data:\n",
    "    for ent in item[\"entities\"]:\n",
    "        if ent[\"label\"] in [\"B-PER\", \"I-PER\"]:\n",
    "            ent[\"label\"] = \"O\"\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Create label maps\n",
    "# -------------------------------\n",
    "unique_labels = set(ent[\"label\"] for item in data for ent in item[\"entities\"])\n",
    "label2id = {label: i for i, label in enumerate(sorted(unique_labels))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Tokenizer\n",
    "# -------------------------------\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Preprocess & align labels\n",
    "# -------------------------------\n",
    "texts, labels = [], []\n",
    "\n",
    "for item in data:\n",
    "    words = [ent[\"word\"] for ent in item[\"entities\"]]\n",
    "    word_labels = [ent[\"label\"] for ent in item[\"entities\"]]\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(words, is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Align labels with subword tokens\n",
    "    aligned_labels = []\n",
    "    for i, word_id in enumerate(tokenized.word_ids(batch_index=0)):\n",
    "        if word_id is None:\n",
    "            aligned_labels.append(-100)  # special tokens\n",
    "        else:\n",
    "            aligned_labels.append(label2id[word_labels[word_id]])\n",
    "\n",
    "    texts.append(tokenized)\n",
    "    labels.append(aligned_labels)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [t[\"input_ids\"] for t in texts],\n",
    "    \"attention_mask\": [t[\"attention_mask\"] for t in texts],\n",
    "    \"labels\": labels\n",
    "})\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Model\n",
    "# -------------------------------\n",
    "num_labels = len(label2id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels,\n",
    "                                                        id2label=id2label, label2id=label2id)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Data collator\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Metrics\n",
    "# -------------------------------\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_preds = [[id2label[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "                  for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    f1 = f1_score(true_labels, true_preds)\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Training arguments\n",
    "# -------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Trainer\n",
    "# -------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Train\n",
    "# -------------------------------\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Save final model\n",
    "# -------------------------------\n",
    "trainer.save_model(\"./ner_model_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aafd02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "Resume Text:\n",
      "\n",
      "John Doe is a software engineer with 5 years of experience in Python, PyTorch, \n",
      "    and NLP. He has designed AI-based resume parsing systems, chatbots, and recommendation engines. \n",
      "    He contributed to open-source ML libraries and published papers on deep learning.\n",
      "\n",
      "NER Predictions:\n",
      "\n",
      "John -> O\n",
      "Doe -> O\n",
      "is -> O\n",
      "a -> O\n",
      "software -> B-ORG\n",
      "engineer -> B-ORG\n",
      "with -> B-ORG\n",
      "5 -> B-ORG\n",
      "years -> B-ORG\n",
      "of -> B-ORG\n",
      "experience -> B-ORG\n",
      "in -> O\n",
      "Python -> O\n",
      ", -> O\n",
      "P -> B-ORG\n",
      ", -> O\n",
      "and -> B-ORG\n",
      "NL -> B-ORG\n",
      ". -> O\n",
      "He -> O\n",
      "has -> B-ORG\n",
      "designed -> B-ORG\n",
      "AI -> O\n",
      "- -> B-ORG\n",
      "based -> B-ORG\n",
      "resume -> O\n",
      "par -> B-ORG\n",
      "systems -> B-ORG\n",
      ", -> O\n",
      "chat -> B-ORG\n",
      ", -> O\n",
      "and -> B-ORG\n",
      "recommendation -> B-ORG\n",
      "engines -> B-ORG\n",
      ". -> O\n",
      "He -> O\n",
      "contributed -> B-ORG\n",
      "to -> B-ORG\n",
      "open -> B-ORG\n",
      "- -> B-ORG\n",
      "source -> B-ORG\n",
      "ML -> O\n",
      "libraries -> B-ORG\n",
      "and -> B-ORG\n",
      "published -> O\n",
      "papers -> B-ORG\n",
      "on -> B-ORG\n",
      "deep -> B-ORG\n",
      "learning -> B-ORG\n",
      ". -> B-ORG\n",
      "\n",
      "===============================\n",
      "Resume Text:\n",
      "\n",
      "Jane Smith is a data scientist specialized in large-scale data processing with Spark and Hadoop. \n",
      "    She implemented predictive models for user behavior analysis, optimized SQL queries, \n",
      "    and led the development of cloud-based analytics pipelines using AWS services.\n",
      "\n",
      "NER Predictions:\n",
      "\n",
      "Jane -> O\n",
      "Smith -> O\n",
      "is -> O\n",
      "a -> B-ORG\n",
      "data -> B-ORG\n",
      "scientist -> B-ORG\n",
      "specialized -> B-ORG\n",
      "in -> B-ORG\n",
      "large -> B-ORG\n",
      "- -> B-ORG\n",
      "scale -> B-ORG\n",
      "data -> B-ORG\n",
      "processing -> O\n",
      "with -> B-ORG\n",
      "Spark -> O\n",
      "and -> B-ORG\n",
      "Had -> B-ORG\n",
      ". -> B-ORG\n",
      "She -> B-ORG\n",
      "implemented -> B-ORG\n",
      "predict -> O\n",
      "models -> O\n",
      "for -> B-ORG\n",
      "user -> B-ORG\n",
      "behavior -> O\n",
      "analysis -> O\n",
      ", -> B-ORG\n",
      "opt -> B-ORG\n",
      "SQL -> B-ORG\n",
      "que -> B-ORG\n",
      ", -> O\n",
      "and -> B-ORG\n",
      "led -> B-ORG\n",
      "the -> B-ORG\n",
      "development -> B-ORG\n",
      "of -> B-ORG\n",
      "cloud -> O\n",
      "- -> B-ORG\n",
      "based -> B-ORG\n",
      "analytics -> O\n",
      "pipeline -> O\n",
      "using -> B-ORG\n",
      "AW -> O\n",
      "services -> O\n",
      ". -> B-ORG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load model and tokenizer\n",
    "# -------------------------------\n",
    "model_name_or_path = \"C:/Users/WIN11/OneDrive/Desktop/resume proj/projwithml/src/ner_model_final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Example tech resumes\n",
    "# -------------------------------\n",
    "texts = [\n",
    "    \"\"\"John Doe is a software engineer with 5 years of experience in Python, PyTorch, \n",
    "    and NLP. He has designed AI-based resume parsing systems, chatbots, and recommendation engines. \n",
    "    He contributed to open-source ML libraries and published papers on deep learning.\"\"\",\n",
    "    \n",
    "    \"\"\"Jane Smith is a data scientist specialized in large-scale data processing with Spark and Hadoop. \n",
    "    She implemented predictive models for user behavior analysis, optimized SQL queries, \n",
    "    and led the development of cloud-based analytics pipelines using AWS services.\"\"\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Run NER safely\n",
    "# -------------------------------\n",
    "for text in texts:\n",
    "    # Tokenize\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    offsets = encoding.pop(\"offset_mapping\")[0]  # remove offsets before passing to model\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "\n",
    "    # Map tokens back to original text\n",
    "    pred_labels = []\n",
    "    current_word = None\n",
    "\n",
    "    for idx, word_id in enumerate(encoding.word_ids(batch_index=0)):\n",
    "        if word_id is None:\n",
    "            continue  # skip special tokens\n",
    "        label_id = predictions[idx].item()\n",
    "        label = model.config.id2label[label_id]\n",
    "\n",
    "        if word_id != current_word:\n",
    "            start, end = offsets[idx].tolist()\n",
    "            word_text = text[start:end]\n",
    "            pred_labels.append((word_text, label))\n",
    "            current_word = word_id\n",
    "\n",
    "    # -------------------------------\n",
    "    # 4. Display results\n",
    "    # -------------------------------\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"Resume Text:\\n\")\n",
    "    print(text)\n",
    "    print(\"\\nNER Predictions:\\n\")\n",
    "    for word, label in pred_labels:\n",
    "        print(f\"{word} -> {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
