{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a55ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 120.01 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 111.03 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 22.22 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 100.01 examples/s]\n",
      "C:\\Users\\WIN11\\AppData\\Local\\Temp\\ipykernel_28524\\1513467905.py:145: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/3 : < :, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "np.int64(6)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    145\u001b[39m trainer = Trainer(\n\u001b[32m    146\u001b[39m     model=model,\n\u001b[32m    147\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# 10. Train\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# 11. Save final model\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m    163\u001b[39m FINAL_MODEL_PATH = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mWIN11\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mOneDrive\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mresume proj\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mprojwithml\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mfourth_phase_model\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2698\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2695\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2697\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2698\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2700\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2702\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2704\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3137\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3135\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3137\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3138\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3140\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3086\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3085\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3086\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3087\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3089\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4254\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4251\u001b[39m start_time = time.time()\n\u001b[32m   4253\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4254\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4255\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4257\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4258\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4262\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4264\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4544\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4542\u001b[39m     eval_set_kwargs[\u001b[33m\"\u001b[39m\u001b[33mlosses\u001b[39m\u001b[33m\"\u001b[39m] = all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4543\u001b[39m     eval_set_kwargs[\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m] = all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4544\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4546\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4547\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4548\u001b[39m     metrics = {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mcompute_metrics\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    108\u001b[39m predictions = predictions.argmax(axis=-\u001b[32m1\u001b[39m)\n\u001b[32m    110\u001b[39m true_labels = [[id2label[l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m label \u001b[38;5;28;01mif\u001b[39;00m l != -\u001b[32m100\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m true_predictions = \u001b[43m[\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    116\u001b[39m results = metric.compute(predictions=true_predictions, references=true_labels)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_precision\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    119\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_recall\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_f1\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    121\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_accuracy\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    122\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    108\u001b[39m predictions = predictions.argmax(axis=-\u001b[32m1\u001b[39m)\n\u001b[32m    110\u001b[39m true_labels = [[id2label[l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m label \u001b[38;5;28;01mif\u001b[39;00m l != -\u001b[32m100\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[32m    111\u001b[39m true_predictions = [\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[43m[\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[32m    114\u001b[39m ]\n\u001b[32m    116\u001b[39m results = metric.compute(predictions=true_predictions, references=true_labels)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_precision\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    119\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_recall\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_f1\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    121\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_accuracy\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    122\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    108\u001b[39m predictions = predictions.argmax(axis=-\u001b[32m1\u001b[39m)\n\u001b[32m    110\u001b[39m true_labels = [[id2label[l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m label \u001b[38;5;28;01mif\u001b[39;00m l != -\u001b[32m100\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[32m    111\u001b[39m true_predictions = [\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     [\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l != -\u001b[32m100\u001b[39m]\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[32m    114\u001b[39m ]\n\u001b[32m    116\u001b[39m results = metric.compute(predictions=true_predictions, references=true_labels)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_precision\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    119\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_recall\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_f1\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    121\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33moverall_accuracy\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    122\u001b[39m }\n",
      "\u001b[31mKeyError\u001b[39m: np.int64(6)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load your Phase 3 trained model\n",
    "# -------------------------------\n",
    "MODEL_PATH = r\"C:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\projwithml\\models\\third_phase_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Check model's label2id mapping\n",
    "print(\"Model labels:\", model.config.label2id)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build your error-corrected dataset\n",
    "# -------------------------------\n",
    "examples = [\n",
    "    {\"tokens\": [\"John\", \"knows\", \"Python\", \"and\", \"Java\"],\n",
    "     \"ner_tags\": [\"O\", \"O\", \"SKILL\", \"O\", \"SKILL\"]},\n",
    "    {\"tokens\": [\"She\", \"worked\", \"with\", \"FastAPI\", \"and\", \"MongoDB\"],\n",
    "     \"ner_tags\": [\"O\", \"O\", \"O\", \"SKILL\", \"O\", \"SKILL\"]},\n",
    "    {\"tokens\": [\"He\", \"is\", \"a\", \"Software\", \"Engineer\", \"at\", \"Google\"],\n",
    "     \"ner_tags\": [\"O\", \"O\", \"O\", \"ROLE\", \"ROLE\", \"O\", \"COMPANY\"]},\n",
    "    {\"tokens\": [\"They\", \"used\", \"NumPy\", \"and\", \"Pandas\", \"for\", \"ML\"],\n",
    "     \"ner_tags\": [\"O\", \"O\", \"SKILL\", \"O\", \"SKILL\", \"O\", \"SKILL\"]}\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Map dataset labels to BIO format (matches Phase 3 model)\n",
    "# -------------------------------\n",
    "def map_labels_to_bio(example):\n",
    "    bio_tags = []\n",
    "    for tag in example[\"ner_tags\"]:\n",
    "        if tag == \"O\":\n",
    "            bio_tags.append(\"O\")\n",
    "        else:\n",
    "            # Single-token entities → use B-TAG\n",
    "            bio_tags.append(f\"B-{tag}\")\n",
    "    example[\"ner_tags\"] = bio_tags\n",
    "    return example\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"tokens\": [ex[\"tokens\"] for ex in examples],\n",
    "    \"ner_tags\": [ex[\"ner_tags\"] for ex in examples]\n",
    "})\n",
    "\n",
    "# Apply BIO mapping\n",
    "dataset = dataset.map(map_labels_to_bio)\n",
    "\n",
    "# Split train/test\n",
    "dataset = dataset.train_test_split(test_size=0.25, seed=42)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Tokenize and align labels\n",
    "# -------------------------------\n",
    "label_list = list(model.config.label2id.keys())\n",
    "label2id = model.config.label2id\n",
    "id2label = model.config.id2label\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, doc_labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[doc_labels[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Data collator and metric\n",
    "# -------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Training Arguments\n",
    "# -------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phase4_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Trainer\n",
    "# -------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Train\n",
    "# -------------------------------\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Save final model\n",
    "# -------------------------------\n",
    "FINAL_MODEL_PATH = r\"C:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\projwithml\\models\\fourth_phase_model\"\n",
    "trainer.save_model(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"✅ Phase 4 training completed. Model saved at {FINAL_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16cbc728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WIN11\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " Michael Rodriguez is employed as a DevOps Engineer at Amazon. He has strong skills in \n",
      "Kubernetes, Docker, Jenkins, and Terraform. Michael is experienced with cloud platforms \n",
      "like AWS and Azure, and is proficient in scripting languages including Python and Bash. \n",
      "He also has knowledge of monitoring tools like Prometheus and Grafana.\n",
      "\n",
      "Extracted Entities:\n",
      "==================================================\n",
      "\n",
      "🎯 ROLES (2):\n",
      "  - DevOps Engineer\n",
      "  - monitoring tools\n",
      "\n",
      "🏢 COMPANIES (6):\n",
      "  - Amazon\n",
      "  - Jenkins\n",
      "  - cloud platforms\n",
      "  - languages\n",
      "  - Bash\n",
      "  - and Grafana\n",
      "\n",
      "💻 SKILLS (8):\n",
      "  - Kubernetes\n",
      "  - Docker\n",
      "  - Terraform\n",
      "  - AWS\n",
      "  - and Azure\n",
      "  - scripting\n",
      "  - Python\n",
      "  - Prometheus\n",
      "\n",
      "📋 All entities (cleaned and filtered):\n",
      "  ROLE     | DevOps Engineer\n",
      "  COMPANY  | Amazon\n",
      "  SKILL    | Kubernetes\n",
      "  SKILL    | Docker\n",
      "  COMPANY  | Jenkins\n",
      "  SKILL    | Terraform\n",
      "  COMPANY  | cloud platforms\n",
      "  SKILL    | AWS\n",
      "  SKILL    | and Azure\n",
      "  SKILL    | scripting\n",
      "  COMPANY  | languages\n",
      "  SKILL    | Python\n",
      "  COMPANY  | Bash\n",
      "  ROLE     | monitoring tools\n",
      "  SKILL    | Prometheus\n",
      "  COMPANY  | and Grafana\n",
      "\n",
      "======================================================================\n",
      "SECOND EXAMPLE:\n",
      "======================================================================\n",
      "Input Text:\n",
      " John Smith is a Lead Software Engineer at Google. He specializes in \n",
      "JavaScript, React, Node.js, and Docker. Previously worked as a Software Developer \n",
      "at Microsoft with experience in C#, .NET, and Azure cloud services.\n",
      "\n",
      "Extracted Entities:\n",
      "==================================================\n",
      "\n",
      "🎯 ROLES (2):\n",
      "  - Lead Software Engineer\n",
      "  - Software Developer\n",
      "\n",
      "🏢 COMPANIES (3):\n",
      "  - Google\n",
      "  - Microsoft\n",
      "  - #,\n",
      "\n",
      "💻 SKILLS (7):\n",
      "  - JavaScript\n",
      "  - React\n",
      "  - Node. js\n",
      "  - Docker\n",
      "  - C\n",
      "  - NET\n",
      "  - Azure cloud services\n",
      "\n",
      "📋 All entities (cleaned and filtered):\n",
      "  ROLE     | Lead Software Engineer\n",
      "  COMPANY  | Google\n",
      "  SKILL    | JavaScript\n",
      "  SKILL    | React\n",
      "  SKILL    | Node. js\n",
      "  SKILL    | Docker\n",
      "  ROLE     | Software Developer\n",
      "  COMPANY  | Microsoft\n",
      "  SKILL    | C\n",
      "  COMPANY  | #,\n",
      "  SKILL    | NET\n",
      "  SKILL    | Azure cloud services\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load trained model\n",
    "# -------------------------------\n",
    "MODEL_PATH = r\"C:\\Users\\WIN11\\OneDrive\\Desktop\\resume proj\\projwithml\\models\\fourth_phase_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "id2label = model.config.id2label\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Entity Cleaner + BIO Merger\n",
    "# -------------------------------\n",
    "def clean_and_merge_entities(tokens, predictions, id2label):\n",
    "    entities = []\n",
    "    current_entity_tokens = []\n",
    "    current_label = None\n",
    "\n",
    "    def stitch_tokens(tok_list):\n",
    "        \"\"\"Join WordPiece tokens into clean text with proper spacing\"\"\"\n",
    "        if not tok_list:\n",
    "            return \"\"\n",
    "        \n",
    "        result = \"\"\n",
    "        \n",
    "        for i, tok in enumerate(tok_list):\n",
    "            if tok.startswith(\"##\"):\n",
    "                # This is a WordPiece continuation - attach directly without space\n",
    "                result += tok[2:]\n",
    "            elif i == 0:\n",
    "                # First token - no space needed\n",
    "                result += tok\n",
    "            elif tok in [\",\", \".\", \":\", \";\", \"'s\"]:\n",
    "                # Punctuation - attach without space\n",
    "                result += tok\n",
    "            else:\n",
    "                # Regular token - add space before\n",
    "                result += \" \" + tok\n",
    "                \n",
    "        return result.strip()\n",
    "\n",
    "    def finalize_entity(tokens, label):\n",
    "        if not tokens: \n",
    "            return None\n",
    "        word = stitch_tokens(tokens).strip()\n",
    "        # remove junk like only punctuation\n",
    "        if re.fullmatch(r\"^[,.\\-;:]+$\", word):\n",
    "            return None\n",
    "        # collapse weird spaces\n",
    "        word = re.sub(r\"\\s+\", \" \", word)\n",
    "        return {\"word\": word, \"entity_group\": label}\n",
    "\n",
    "    def is_wordpiece_continuation(token):\n",
    "        \"\"\"Check if token is a WordPiece continuation\"\"\"\n",
    "        return token.startswith(\"##\")\n",
    "\n",
    "    def should_continue_entity(token, current_label, predicted_label):\n",
    "        \"\"\"\n",
    "        Decide if token should continue current entity based on context\n",
    "        \"\"\"\n",
    "        if not current_label:\n",
    "            return False\n",
    "            \n",
    "        # If it's a WordPiece continuation, it should continue the current entity\n",
    "        if is_wordpiece_continuation(token):\n",
    "            return True\n",
    "            \n",
    "        # Handle punctuation that should stay with entities\n",
    "        if token in [\",\", \".\", \"'s\", \"'\", \"s\"] and current_label:\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    # Enhanced BIO merging with WordPiece awareness\n",
    "    for i, (token, pred_id) in enumerate(zip(tokens, predictions)):\n",
    "        label = id2label[pred_id]\n",
    "\n",
    "        if token in tokenizer.all_special_tokens:\n",
    "            continue\n",
    "\n",
    "        # Parse label\n",
    "        if \"-\" in label:\n",
    "            prefix, entity_type = label.split(\"-\", 1)\n",
    "        else:\n",
    "            prefix, entity_type = \"B\", label if label != \"O\" else None\n",
    "\n",
    "        # Decide how to handle this token\n",
    "        if label == \"O\" and not should_continue_entity(token, current_label, label):\n",
    "            # End current entity and don't start a new one\n",
    "            ent = finalize_entity(current_entity_tokens, current_label)\n",
    "            if ent:\n",
    "                entities.append(ent)\n",
    "            current_entity_tokens, current_label = [], None\n",
    "            \n",
    "        elif should_continue_entity(token, current_label, label):\n",
    "            # Continue current entity regardless of predicted label\n",
    "            current_entity_tokens.append(token)\n",
    "            \n",
    "        elif prefix == \"B\" or (prefix == \"I\" and entity_type != current_label):\n",
    "            # Start new entity\n",
    "            ent = finalize_entity(current_entity_tokens, current_label)\n",
    "            if ent:\n",
    "                entities.append(ent)\n",
    "            current_entity_tokens = [token]\n",
    "            current_label = entity_type\n",
    "            \n",
    "        elif prefix == \"I\" and entity_type == current_label:\n",
    "            # Continue current entity\n",
    "            current_entity_tokens.append(token)\n",
    "            \n",
    "        else:\n",
    "            # Fallback: start new entity\n",
    "            ent = finalize_entity(current_entity_tokens, current_label)\n",
    "            if ent:\n",
    "                entities.append(ent)\n",
    "            current_entity_tokens = [token] if label != \"O\" else []\n",
    "            current_label = entity_type if label != \"O\" else None\n",
    "\n",
    "    # Finalize last entity\n",
    "    ent = finalize_entity(current_entity_tokens, current_label)\n",
    "    if ent:\n",
    "        entities.append(ent)\n",
    "\n",
    "    return post_process_entities(entities)\n",
    "\n",
    "\n",
    "def post_process_entities(entities):\n",
    "    \"\"\"\n",
    "    Post-process entities to extract SKILLS, COMPANY, and ROLE\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return entities\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    # Define patterns for better classification\n",
    "    skill_keywords = {\n",
    "        'python', 'java', 'tensorflow', 'docker', 'kubernetes', 'git', \n",
    "        'aws', 'react', 'nodejs', 'javascript', 'c++', 'c#', 'ruby',\n",
    "        'angular', 'vue', 'mysql', 'postgresql', 'mongodb', 'redis',\n",
    "        'html', 'css', 'sql', 'nosql', 'flask', 'django', 'spring',\n",
    "        'pytorch', 'scikit-learn', 'pandas', 'numpy', 'matplotlib',\n",
    "        'spark', 'hadoop', 'machine learning', 'data processing',\n",
    "        'google cloud', 'azure', 'tableau', 'power bi'\n",
    "    }\n",
    "    \n",
    "    role_keywords = {\n",
    "        'scientist', 'engineer', 'developer', 'analyst', 'manager', \n",
    "        'director', 'lead', 'senior', 'junior', 'consultant', \n",
    "        'architect', 'specialist', 'coordinator', 'supervisor',\n",
    "        'data scientist', 'software engineer', 'product manager',\n",
    "        'business analyst', 'project manager', 'team lead'\n",
    "    }\n",
    "    \n",
    "    company_keywords = {\n",
    "        'netflix', 'google', 'microsoft', 'amazon', 'apple', 'meta',\n",
    "        'facebook', 'tesla', 'uber', 'airbnb', 'spotify', 'adobe',\n",
    "        'salesforce', 'oracle', 'ibm', 'intel', 'nvidia'\n",
    "    }\n",
    "    \n",
    "    # Skip educational institutions\n",
    "    educational_institutions = {\n",
    "        'mit', 'stanford', 'harvard', 'berkeley', 'cmu', 'caltech',\n",
    "        'university', 'college', 'institute', 'school'\n",
    "    }\n",
    "    \n",
    "    for entity in entities:\n",
    "        word = entity[\"word\"].strip()\n",
    "        entity_type = entity[\"entity_group\"]\n",
    "        \n",
    "        # Skip empty or punctuation-only entities\n",
    "        if not word or re.fullmatch(r\"^[,.\\-;:'\\s]+$\", word):\n",
    "            continue\n",
    "            \n",
    "        # Clean up the word\n",
    "        word = re.sub(r'\\s+', ' ', word)  # Normalize spaces\n",
    "        word = re.sub(r'\\s*[,.:;]\\s*$', '', word)  # Remove trailing punctuation\n",
    "        word = re.sub(r'^\\s*[,.:;]\\s*', '', word)  # Remove leading punctuation\n",
    "        \n",
    "        # Skip if still empty after cleaning\n",
    "        if not word.strip():\n",
    "            continue\n",
    "            \n",
    "        word_lower = word.lower().strip()\n",
    "        \n",
    "        # Skip educational institutions\n",
    "        if any(edu in word_lower for edu in educational_institutions):\n",
    "            continue\n",
    "        \n",
    "        # Skip obviously wrong entities (like single words that don't make sense)\n",
    "        if word_lower in ['big', 'sarah', 'she', 'holds', 'certifications', 'in']:\n",
    "            continue\n",
    "            \n",
    "        # Reclassify based on content\n",
    "        new_entity_type = entity_type\n",
    "        \n",
    "        # Check if it's a skill\n",
    "        if any(skill in word_lower for skill in skill_keywords):\n",
    "            new_entity_type = \"SKILL\"\n",
    "        \n",
    "        # Check if it's a role\n",
    "        elif any(role in word_lower for role in role_keywords):\n",
    "            new_entity_type = \"ROLE\"\n",
    "            \n",
    "        # Check if it's a company\n",
    "        elif any(company in word_lower for company in company_keywords):\n",
    "            new_entity_type = \"COMPANY\"\n",
    "        \n",
    "        # Additional role detection based on common patterns\n",
    "        if re.search(r'\\b(senior|junior|lead|principal|chief)\\s+\\w+', word_lower):\n",
    "            new_entity_type = \"ROLE\"\n",
    "        \n",
    "        # Only keep relevant entities\n",
    "        if new_entity_type in [\"SKILL\", \"COMPANY\", \"ROLE\"]:\n",
    "            processed.append({\n",
    "                \"word\": word,\n",
    "                \"entity_group\": new_entity_type\n",
    "            })\n",
    "    \n",
    "    return remove_duplicates(processed)\n",
    "\n",
    "\n",
    "def remove_duplicates(entities):\n",
    "    \"\"\"Remove duplicate entities while preserving order\"\"\"\n",
    "    seen = set()\n",
    "    unique_entities = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        # Create a key based on normalized word and entity type\n",
    "        key = (entity[\"word\"].lower().strip(), entity[\"entity_group\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_entities.append(entity)\n",
    "    \n",
    "    return unique_entities\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Predict Function\n",
    "# -------------------------------\n",
    "def extract_entities(text: str, debug=False):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nDEBUG: Token-Label pairs:\")\n",
    "        for i, (token, pred_id) in enumerate(zip(tokens, predictions)):\n",
    "            if token not in tokenizer.all_special_tokens:\n",
    "                print(f\"{i:2d}: {token:15} -> {id2label[pred_id]:15}\")\n",
    "        print()\n",
    "\n",
    "    return clean_and_merge_entities(tokens, predictions, id2label)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Enhanced Output Function\n",
    "# -------------------------------\n",
    "def display_extracted_entities(entities, text):\n",
    "    \"\"\"Display entities in a clean, organized format\"\"\"\n",
    "    print(\"Input Text:\\n\", text)\n",
    "    print(\"\\nExtracted Entities:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Group entities by type\n",
    "    skills = [ent for ent in entities if ent[\"entity_group\"] == \"SKILL\"]\n",
    "    companies = [ent for ent in entities if ent[\"entity_group\"] == \"COMPANY\"]\n",
    "    roles = [ent for ent in entities if ent[\"entity_group\"] == \"ROLE\"]\n",
    "    \n",
    "    print(f\"\\n🎯 ROLES ({len(roles)}):\")\n",
    "    if roles:\n",
    "        for role in roles:\n",
    "            print(f\"  - {role['word']}\")\n",
    "    else:\n",
    "        print(\"  - No roles detected\")\n",
    "    \n",
    "    print(f\"\\n🏢 COMPANIES ({len(companies)}):\")\n",
    "    if companies:\n",
    "        for company in companies:\n",
    "            print(f\"  - {company['word']}\")\n",
    "    else:\n",
    "        print(\"  - No companies detected\")\n",
    "    \n",
    "    print(f\"\\n💻 SKILLS ({len(skills)}):\")\n",
    "    if skills:\n",
    "        for skill in skills:\n",
    "            print(f\"  - {skill['word']}\")\n",
    "    else:\n",
    "        print(\"  - No skills detected\")\n",
    "    \n",
    "    print(f\"\\n📋 All entities (cleaned and filtered):\")\n",
    "    for ent in entities:\n",
    "        print(f\"  {ent['entity_group']:8} | {ent['word']}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Example Usage\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"Michael Rodriguez is employed as a DevOps Engineer at Amazon. He has strong skills in \n",
    "Kubernetes, Docker, Jenkins, and Terraform. Michael is experienced with cloud platforms \n",
    "like AWS and Azure, and is proficient in scripting languages including Python and Bash. \n",
    "He also has knowledge of monitoring tools like Prometheus and Grafana.\"\"\"\n",
    "\n",
    "    entities = extract_entities(text, debug=False)\n",
    "    display_extracted_entities(entities, text)\n",
    "    \n",
    "    # Test with another example\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SECOND EXAMPLE:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    text2 = \"\"\"John Smith is a Lead Software Engineer at Google. He specializes in \n",
    "JavaScript, React, Node.js, and Docker. Previously worked as a Software Developer \n",
    "at Microsoft with experience in C#, .NET, and Azure cloud services.\"\"\"\n",
    "    \n",
    "    entities2 = extract_entities(text2, debug=False)\n",
    "    display_extracted_entities(entities2, text2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
